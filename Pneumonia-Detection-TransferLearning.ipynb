{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:28.172718Z",
     "iopub.status.busy": "2020-12-15T19:29:28.171866Z",
     "iopub.status.idle": "2020-12-15T19:29:35.607153Z",
     "shell.execute_reply": "2020-12-15T19:29:35.607933Z"
    },
    "papermill": {
     "duration": 7.498071,
     "end_time": "2020-12-15T19:29:35.608203",
     "exception": false,
     "start_time": "2020-12-15T19:29:28.110132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:36.288810Z",
     "iopub.status.busy": "2020-12-15T19:29:36.287866Z",
     "iopub.status.idle": "2020-12-15T19:29:38.195380Z",
     "shell.execute_reply": "2020-12-15T19:29:38.195974Z"
    },
    "papermill": {
     "duration": 1.987021,
     "end_time": "2020-12-15T19:29:38.196118",
     "exception": false,
     "start_time": "2020-12-15T19:29:36.209097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take directories of the corresponding datasets\n",
    "train_data = \"chest_xray/train\"\n",
    "test_data = \"chest_xray/test\"\n",
    "val_data = \"chest_xray/val\"\n",
    "\n",
    "# get the count of respective images in all datasets\n",
    "train_num_pneumonia = len(os.listdir(os.path.join(train_data, 'PNEUMONIA')))\n",
    "train_num_normal = len(os.listdir(os.path.join(train_data, 'NORMAL')))\n",
    "\n",
    "test_num_pneumonia=len(os.listdir(os.path.join(test_data, 'PNEUMONIA')))\n",
    "test_num_normal=len(os.listdir(os.path.join(test_data, 'NORMAL')))\n",
    "\n",
    "val_num_pnemonia=len(os.listdir(os.path.join(val_data, 'PNEUMONIA')))\n",
    "val_num_normal=len(os.listdir(os.path.join(val_data, 'NORMAL')))\n",
    "\n",
    "# print all the counts of images\n",
    "print(\"Train dataset:\")\n",
    "print(f\"The number of Pnemonia images in training dataset={train_num_pneumonia}\")\n",
    "print(f\"The number of Normal images in training dataset={train_num_normal}\")\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(f\"The number of Pnemonia images in testing dataset={test_num_pneumonia}\")\n",
    "print(f\"The number of Pnemonia images in testing dataset={test_num_normal}\")\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "print(f\"The number of Pnemonia images in validation dataset={val_num_pnemonia}\")\n",
    "print(f\"The number of Pnemonia images in validation dataset={val_num_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Pneumonia Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the images of Pneumonia chest X-rays\n",
    "pneumonia = os.listdir(\"chest_xray/train/PNEUMONIA\")\n",
    "pneumonia_dataset = \"chest_xray/train/PNEUMONIA\"\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    img = plt.imread(os.path.join(pneumonia_dataset, pneumonia[i]))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Normal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:38.342007Z",
     "iopub.status.busy": "2020-12-15T19:29:38.340616Z",
     "iopub.status.idle": "2020-12-15T19:29:40.768517Z",
     "shell.execute_reply": "2020-12-15T19:29:40.769185Z"
    },
    "papermill": {
     "duration": 2.503207,
     "end_time": "2020-12-15T19:29:40.769366",
     "exception": false,
     "start_time": "2020-12-15T19:29:38.266159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot some of the images of Normal chest X-rays\n",
    "normal = os.listdir(\"chest_xray/train/NORMAL\")\n",
    "normal_dataset = \"chest_xray/train/NORMAL\"\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    img = plt.imread(os.path.join(normal_dataset, normal[i]))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of data points of both classes in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bar graph for the 2 classes in training dataset\n",
    "plt.bar(['NORMAL', 'PNEUMONIA'], [train_num_normal, train_num_pneumonia], color=['g', 'r'])\n",
    "plt.title('Number of Data Points in Each Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Data Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of data points of both classes in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bar graph for the 2 classes in training dataset\n",
    "plt.bar(['NORMAL', 'PNEUMONIA'], [test_num_normal, test_num_pneumonia], color=['g', 'r'])\n",
    "plt.title('Number of Data Points in each class of testing dataset')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Data Points')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of the Image in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:40.974574Z",
     "iopub.status.busy": "2020-12-15T19:29:40.973531Z",
     "iopub.status.idle": "2020-12-15T19:29:41.473261Z",
     "shell.execute_reply": "2020-12-15T19:29:41.473842Z"
    },
    "papermill": {
     "duration": 0.608404,
     "end_time": "2020-12-15T19:29:41.474025",
     "exception": false,
     "start_time": "2020-12-15T19:29:40.865621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# get one of the image of normal chest X-ray and plot \n",
    "normal_data = \"chest_xray/train/NORMAL\"\n",
    "normal_image_path = glob.glob(normal_data + \"/*.jpeg\")[0]\n",
    "sample_image = plt.imread(normal_image_path)\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Raw Chest X Ray Image')\n",
    "\n",
    "# print the characteristics of the image\n",
    "print(\"Image dimensions: {} pixels width x {} pixels height\".format(sample_image.shape[0], sample_image.shape[1]))\n",
    "print(\"Maximum pixel value: {:.4f}\".format(sample_image.max()))\n",
    "print(\"Minimum pixel value: {:.4f}\".format(sample_image.min()))\n",
    "print(\"Mean pixel value: {:.4f}\".format(sample_image.mean()))\n",
    "print(\"Standard deviation of pixel values: {:.4f}\".format(sample_image.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Pixel Intensities in the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:41.826931Z",
     "iopub.status.busy": "2020-12-15T19:29:41.825840Z",
     "iopub.status.idle": "2020-12-15T19:29:42.404421Z",
     "shell.execute_reply": "2020-12-15T19:29:42.406544Z"
    },
    "papermill": {
     "duration": 0.675379,
     "end_time": "2020-12-15T19:29:42.406797",
     "exception": false,
     "start_time": "2020-12-15T19:29:41.731418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the pixel intensities of the sample image\n",
    "pixel_mean = np.mean(sample_image)\n",
    "pixel_std = np.std(sample_image)\n",
    "\n",
    "sns.histplot(sample_image.ravel(), kde=False)\n",
    "plt.axvline(x=pixel_mean, color='r', linestyle='--', label=f\"Pixel Mean {pixel_mean:.4f}\")\n",
    "plt.axvline(x=pixel_std, color='g', linestyle='--', label=f\"Pixel Standard Deviation {pixel_std:.4f}\")\n",
    "plt.legend(loc='upper center')\n",
    "plt.title('Distribution of Pixel Intensities in the Image')\n",
    "plt.xlabel('Intensity of the pixel')\n",
    "plt.ylabel('# Image Pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Data Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:43.098799Z",
     "iopub.status.busy": "2020-12-15T19:29:43.097860Z",
     "iopub.status.idle": "2020-12-15T19:29:43.160808Z",
     "shell.execute_reply": "2020-12-15T19:29:43.159532Z"
    },
    "papermill": {
     "duration": 0.212008,
     "end_time": "2020-12-15T19:29:43.160964",
     "exception": false,
     "start_time": "2020-12-15T19:29:42.948956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator for data prepocessing and data augmentation\n",
    "image_generator = ImageDataGenerator(\n",
    "    rotation_range=30,  # increasing the rotation range to 30 degrees\n",
    "    width_shift_range=0.2,  # increasing the width shift range to 20%\n",
    "    shear_range=0.2,  # increasing shear range to 20%\n",
    "    zoom_range=0.2,  # increasing the zoom range to 20%\n",
    "    samplewise_center=True,  # enabling the samplewise centering\n",
    "    samplewise_std_normalization=True,  # enabling the samplewise std normalization # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "    vertical_flip=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test and Validation Image Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:43.650349Z",
     "iopub.status.busy": "2020-12-15T19:29:43.649529Z",
     "iopub.status.idle": "2020-12-15T19:29:45.988934Z",
     "shell.execute_reply": "2020-12-15T19:29:45.987864Z"
    },
    "papermill": {
     "duration": 2.438479,
     "end_time": "2020-12-15T19:29:45.989061",
     "exception": false,
     "start_time": "2020-12-15T19:29:43.550582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the images to their corresponding variables using different image generators for each\n",
    "train = image_generator.flow_from_directory(train_data, \n",
    "                                            batch_size=8, \n",
    "                                            shuffle=True, \n",
    "                                            class_mode='binary',\n",
    "                                            target_size=(180, 180))\n",
    "\n",
    "test = image_generator.flow_from_directory(test_data, \n",
    "                                            batch_size=8, \n",
    "                                            shuffle=False, \n",
    "                                            class_mode='binary',\n",
    "                                            target_size=(180, 180))\n",
    "\n",
    "validation = image_generator.flow_from_directory(val_data, \n",
    "                                                batch_size=8, \n",
    "                                                shuffle=False, \n",
    "                                                class_mode='binary',\n",
    "                                                target_size=(180, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Preprocessed Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sample augmented image for visualization\n",
    "sns.set_style('white')\n",
    "generated_image, label = train.__getitem__(0)\n",
    "plt.imshow(generated_image[0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Raw Chest X Ray Image')\n",
    "\n",
    "print(f\"The dimensions of the image are {generated_image.shape[1]} pixels width and {generated_image.shape[2]} pixels height, one single color channel.\")\n",
    "print(f\"The maximum pixel value is {generated_image.max():.4f} and the minimum is {generated_image.min():.4f}\")\n",
    "print(f\"The mean value of the pixels is {generated_image.mean():.4f} and the standard deviation is {generated_image.std():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Network Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN model consists of convolutional layers, pooling layers, fully connected layers, and finally more convolutional layers. By introducing filters on the input image to discover patterns and features like edges, curves, and forms, the convolutional layers conduct feature extraction. The pooling layers serve to lower the computational complexity of the model by shrinking the size of the feature maps produced by the convolutional layers. \n",
    "\n",
    "\n",
    "The final classification decision is subsequently made by one or more fully connected layers using the output of the final convolutional and pooling layers, which has been flattened and connected. The retrieved features are used by the fully connected layers to determine whether or not pneumonia is present in the chest X-ray pictures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:47.989740Z",
     "iopub.status.busy": "2020-12-15T19:29:47.989083Z",
     "iopub.status.idle": "2020-12-15T19:29:47.993318Z",
     "shell.execute_reply": "2020-12-15T19:29:47.993895Z"
    },
    "papermill": {
     "duration": 0.101751,
     "end_time": "2020-12-15T19:29:47.994063",
     "exception": false,
     "start_time": "2020-12-15T19:29:47.892312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate class weights and print them\n",
    "class_0_wgt = train_num_pneumonia / (train_num_normal + train_num_pneumonia)\n",
    "class_1_wgt = train_num_normal / (train_num_normal + train_num_pneumonia)\n",
    "\n",
    "class_wgt = {0: class_0_wgt, 1: class_1_wgt}\n",
    "\n",
    "print(f\"class 0 weight: {class_0_wgt:.2f}\")\n",
    "print(f\"class 1 weight: {class_1_wgt:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:48.192395Z",
     "iopub.status.busy": "2020-12-15T19:29:48.191679Z",
     "iopub.status.idle": "2020-12-15T19:29:52.286979Z",
     "shell.execute_reply": "2020-12-15T19:29:52.285458Z"
    },
    "papermill": {
     "duration": 4.203503,
     "end_time": "2020-12-15T19:29:52.287101",
     "exception": false,
     "start_time": "2020-12-15T19:29:48.083598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the CNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "model = Sequential()\n",
    "\n",
    "# add Multiple layers of convolutional, pooling, and dense (fully-connected) layers with batch normalization\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(180, 180, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(180, 180, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model using below parameters\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:29:52.685161Z",
     "iopub.status.busy": "2020-12-15T19:29:52.684430Z",
     "iopub.status.idle": "2020-12-15T19:33:18.326026Z",
     "shell.execute_reply": "2020-12-15T19:33:18.325316Z"
    },
    "papermill": {
     "duration": 205.738782,
     "end_time": "2020-12-15T19:33:18.326163",
     "exception": false,
     "start_time": "2020-12-15T19:29:52.587381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "cnn_model = model.fit(\n",
    "    train, \n",
    "    epochs=10,\n",
    "    validation_data=validation_generator, \n",
    "    class_weight=class_wgt,\n",
    "    steps_per_epoch=100,\n",
    "    validation_steps=25,\n",
    "    verbose=1,  # Add this argument to see the progress of the training \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:33:19.381801Z",
     "iopub.status.busy": "2020-12-15T19:33:19.380755Z",
     "iopub.status.idle": "2020-12-15T19:33:20.168104Z",
     "shell.execute_reply": "2020-12-15T19:33:20.167508Z"
    },
    "papermill": {
     "duration": 1.306952,
     "end_time": "2020-12-15T19:33:20.168225",
     "exception": false,
     "start_time": "2020-12-15T19:33:18.861273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(cnn_model.history['loss'], 'g-o', label='Training loss')\n",
    "plt.plot(cnn_model.history['val_loss'], 'r-o', label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss Evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(cnn_model.history['accuracy'], 'go-', label='Training accuracy')\n",
    "plt.plot(cnn_model.history['val_accuracy'], 'ro-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Accuracy Evaluation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:33:21.295337Z",
     "iopub.status.busy": "2020-12-15T19:33:21.294454Z",
     "iopub.status.idle": "2020-12-15T19:35:31.513410Z",
     "shell.execute_reply": "2020-12-15T19:35:31.512678Z"
    },
    "papermill": {
     "duration": 130.669034,
     "end_time": "2020-12-15T19:35:31.513555",
     "exception": false,
     "start_time": "2020-12-15T19:33:20.844521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "test_acc = model.evaluate(test)\n",
    "# calculate accuarcy\n",
    "print(f\"Test Accuracy: {test_acc[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with train data\n",
    "train_acc = model.evaluate(train)\n",
    "# calculate accuarcy\n",
    "print(f\"Train Accuracy: {train_acc[1] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:35:34.262508Z",
     "iopub.status.busy": "2020-12-15T19:35:34.261624Z",
     "iopub.status.idle": "2020-12-15T19:35:46.167155Z",
     "shell.execute_reply": "2020-12-15T19:35:46.166442Z"
    },
    "papermill": {
     "duration": 12.940536,
     "end_time": "2020-12-15T19:35:46.167273",
     "exception": false,
     "start_time": "2020-12-15T19:35:33.226737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# predict the output with test data using trained model\n",
    "cnn_pred = model.predict(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "cm=confusion_matrix(test.classes, cnn_pred > 0.5)\n",
    "print(cm)\n",
    "\n",
    "# compute classification report with threshold 0.5 and print\n",
    "report=classification_report(test.classes, cnn_pred > 0.5, output_dict=True)\n",
    "df=pd.DataFrame(report)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:35:47.733994Z",
     "iopub.status.busy": "2020-12-15T19:35:47.733095Z",
     "iopub.status.idle": "2020-12-15T19:35:47.758760Z",
     "shell.execute_reply": "2020-12-15T19:35:47.758151Z"
    },
    "papermill": {
     "duration": 0.806183,
     "end_time": "2020-12-15T19:35:47.758866",
     "exception": false,
     "start_time": "2020-12-15T19:35:46.952683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.7 and print\n",
    "print(confusion_matrix(test.classes, cnn_pred > 0.7))\n",
    "\n",
    "# compute classification report with threshold 0.7 and print\n",
    "pd.DataFrame(classification_report(test.classes, cnn_pred > 0.7, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.804427,
     "end_time": "2020-12-15T19:35:49.372786",
     "exception": false,
     "start_time": "2020-12-15T19:35:48.568359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transfer Learning - Existing Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-trained convolutional neural network (CNN) models DenseNet121, ResNet50, InceptionV3, InceptionResNetV2, MobileNetV2, and EfficientNetB0 have all been trained to categorize images using big datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:35:51.003420Z",
     "iopub.status.busy": "2020-12-15T19:35:51.002671Z",
     "iopub.status.idle": "2020-12-15T19:35:58.601170Z",
     "shell.execute_reply": "2020-12-15T19:35:58.600244Z"
    },
    "papermill": {
     "duration": 8.407444,
     "end_time": "2020-12-15T19:35:58.601337",
     "exception": false,
     "start_time": "2020-12-15T19:35:50.193893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# build the model using Densenet121 from keras\n",
    "base_model = DenseNet121(input_shape=(180, 180, 3), include_top=False, weights='imagenet', pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:36:00.367344Z",
     "iopub.status.busy": "2020-12-15T19:36:00.366516Z",
     "iopub.status.idle": "2020-12-15T19:36:00.370750Z",
     "shell.execute_reply": "2020-12-15T19:36:00.371276Z"
    },
    "papermill": {
     "duration": 0.812815,
     "end_time": "2020-12-15T19:36:00.371419",
     "exception": false,
     "start_time": "2020-12-15T19:35:59.558604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the number of layers in the model\n",
    "layers = base_model.layers\n",
    "print(f\"The model has {len(layers)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:36:02.094819Z",
     "iopub.status.busy": "2020-12-15T19:36:02.092294Z",
     "iopub.status.idle": "2020-12-15T19:36:02.097851Z",
     "shell.execute_reply": "2020-12-15T19:36:02.097019Z"
    },
    "papermill": {
     "duration": 0.922904,
     "end_time": "2020-12-15T19:36:02.098031",
     "exception": false,
     "start_time": "2020-12-15T19:36:01.175127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the input and output shapes\n",
    "print(f\"The input shape {base_model.input}\")\n",
    "print(f\"The output shape {base_model.output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:36:03.745347Z",
     "iopub.status.busy": "2020-12-15T19:36:03.744244Z",
     "iopub.status.idle": "2020-12-15T19:36:10.827275Z",
     "shell.execute_reply": "2020-12-15T19:36:10.826580Z"
    },
    "papermill": {
     "duration": 7.893372,
     "end_time": "2020-12-15T19:36:10.827389",
     "exception": false,
     "start_time": "2020-12-15T19:36:02.934017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and add pooling layers\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet')\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:36:12.434221Z",
     "iopub.status.busy": "2020-12-15T19:36:12.433496Z",
     "iopub.status.idle": "2020-12-15T19:39:28.850417Z",
     "shell.execute_reply": "2020-12-15T19:39:28.851777Z"
    },
    "papermill": {
     "duration": 197.238412,
     "end_time": "2020-12-15T19:39:28.852047",
     "exception": false,
     "start_time": "2020-12-15T19:36:11.613635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "densenet_model = model.fit(\n",
    "    train, \n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_wgt,\n",
    "    steps_per_epoch=100,\n",
    "    validation_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:39:31.236048Z",
     "iopub.status.busy": "2020-12-15T19:39:31.234957Z",
     "iopub.status.idle": "2020-12-15T19:39:32.049770Z",
     "shell.execute_reply": "2020-12-15T19:39:32.051030Z"
    },
    "papermill": {
     "duration": 1.971106,
     "end_time": "2020-12-15T19:39:32.051247",
     "exception": false,
     "start_time": "2020-12-15T19:39:30.080141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(densenet_model.history['loss'], label='Loss')\n",
    "plt.plot(densenet_model.history['val_loss'], label='Val_Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evaluation')\n",
    "\n",
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(densenet_model.history['accuracy'], label='Accuracy')\n",
    "plt.plot(densenet_model.history['val_accuracy'], label='Val_Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:39:34.473044Z",
     "iopub.status.busy": "2020-12-15T19:39:34.472265Z",
     "iopub.status.idle": "2020-12-15T19:41:27.376971Z",
     "shell.execute_reply": "2020-12-15T19:41:27.378282Z"
    },
    "papermill": {
     "duration": 114.044983,
     "end_time": "2020-12-15T19:41:27.378466",
     "exception": false,
     "start_time": "2020-12-15T19:39:33.333483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "evaluation = model.evaluate(test)\n",
    "# calculate accuarcy\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with test data\n",
    "evaluation = model.evaluate(train)\n",
    "# calculate accuarcy\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:41:33.828391Z",
     "iopub.status.busy": "2020-12-15T19:41:33.827640Z",
     "iopub.status.idle": "2020-12-15T19:41:49.062330Z",
     "shell.execute_reply": "2020-12-15T19:41:49.060707Z"
    },
    "papermill": {
     "duration": 16.687864,
     "end_time": "2020-12-15T19:41:49.062459",
     "exception": false,
     "start_time": "2020-12-15T19:41:32.374595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict the output with test data using trained model\n",
    "pred = model.predict(test, steps=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:41:51.878586Z",
     "iopub.status.busy": "2020-12-15T19:41:51.877721Z",
     "iopub.status.idle": "2020-12-15T19:41:51.902324Z",
     "shell.execute_reply": "2020-12-15T19:41:51.902766Z"
    },
    "papermill": {
     "duration": 1.426694,
     "end_time": "2020-12-15T19:41:51.902911",
     "exception": false,
     "start_time": "2020-12-15T19:41:50.476217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "print(confusion_matrix(test.classes, pred > 0.5))\n",
    "# compute classification report with threshold 0.5 and print\n",
    "pd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.042948,
     "end_time": "2020-12-15T19:47:14.693362",
     "exception": false,
     "start_time": "2020-12-15T19:47:12.650414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:47:19.108462Z",
     "iopub.status.busy": "2020-12-15T19:47:19.107577Z",
     "iopub.status.idle": "2020-12-15T19:47:24.219643Z",
     "shell.execute_reply": "2020-12-15T19:47:24.218810Z"
    },
    "papermill": {
     "duration": 7.436985,
     "end_time": "2020-12-15T19:47:24.219764",
     "exception": false,
     "start_time": "2020-12-15T19:47:16.782779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "\n",
    "# build the model using ResNet50 from keras\n",
    "resnet_base_model = ResNet50(input_shape=(180,180,3), include_top=False, weights='imagenet')\n",
    "resnet_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Multiple layers of pooling, dropout and dense (fully-connected) layers with batch normalization\n",
    "resnet_model = Sequential([\n",
    "        resnet_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1,activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:47:33.391497Z",
     "iopub.status.busy": "2020-12-15T19:47:33.384216Z",
     "iopub.status.idle": "2020-12-15T19:47:36.006838Z",
     "shell.execute_reply": "2020-12-15T19:47:36.006234Z"
    },
    "papermill": {
     "duration": 5.120215,
     "end_time": "2020-12-15T19:47:36.006983",
     "exception": false,
     "start_time": "2020-12-15T19:47:30.886768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# define evaluation metrics\n",
    "METRICS = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "# compile the model\n",
    "resnet_model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:47:40.360528Z",
     "iopub.status.busy": "2020-12-15T19:47:40.359741Z",
     "iopub.status.idle": "2020-12-15T19:50:50.206236Z",
     "shell.execute_reply": "2020-12-15T19:50:50.205283Z"
    },
    "papermill": {
     "duration": 192.091834,
     "end_time": "2020-12-15T19:50:50.206363",
     "exception": false,
     "start_time": "2020-12-15T19:47:38.114529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "rn_model = resnet_model.fit(train,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          class_weight=class_wgt,\n",
    "          steps_per_epoch=100,\n",
    "          validation_steps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:50:55.040901Z",
     "iopub.status.busy": "2020-12-15T19:50:55.039810Z",
     "iopub.status.idle": "2020-12-15T19:50:55.573707Z",
     "shell.execute_reply": "2020-12-15T19:50:55.574323Z"
    },
    "papermill": {
     "duration": 2.96698,
     "end_time": "2020-12-15T19:50:55.574484",
     "exception": false,
     "start_time": "2020-12-15T19:50:52.607504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rn_model.history['loss'], label='Loss')\n",
    "plt.plot(rn_model.history['val_loss'], label='Val_Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rn_model.history['accuracy'], label='Accuracy')\n",
    "plt.plot(rn_model.history['val_accuracy'], label='Val_Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:51:00.758079Z",
     "iopub.status.busy": "2020-12-15T19:51:00.757429Z",
     "iopub.status.idle": "2020-12-15T19:52:57.427549Z",
     "shell.execute_reply": "2020-12-15T19:52:57.428215Z"
    },
    "papermill": {
     "duration": 119.222206,
     "end_time": "2020-12-15T19:52:57.428368",
     "exception": false,
     "start_time": "2020-12-15T19:50:58.206162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "evaluation = resnet_model.evaluate(test)\n",
    "# calculate accuracy\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with train data\n",
    "evaluation = resnet_model.evaluate(train)\n",
    "# calculate accuracy\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output with test data using trained model\n",
    "pred = resnet_model.predict(test, steps=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "print(confusion_matrix(test.classes, pred > 0.5))\n",
    "# compute classification report with threshold 0.5 and print\n",
    "pd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.811575,
     "end_time": "2020-12-15T19:53:03.194168",
     "exception": false,
     "start_time": "2020-12-15T19:53:00.382593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inception Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:53:09.123684Z",
     "iopub.status.busy": "2020-12-15T19:53:09.122682Z",
     "iopub.status.idle": "2020-12-15T19:53:16.611047Z",
     "shell.execute_reply": "2020-12-15T19:53:16.610462Z"
    },
    "papermill": {
     "duration": 10.551292,
     "end_time": "2020-12-15T19:53:16.611199",
     "exception": false,
     "start_time": "2020-12-15T19:53:06.059907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "\n",
    "# build the model using InceptionV3 from keras\n",
    "inception_base_model = InceptionV3(input_shape=(180,180,3),include_top=False,weights='imagenet')\n",
    "inception_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:53:22.525711Z",
     "iopub.status.busy": "2020-12-15T19:53:22.524743Z",
     "iopub.status.idle": "2020-12-15T19:53:25.684765Z",
     "shell.execute_reply": "2020-12-15T19:53:25.683842Z"
    },
    "papermill": {
     "duration": 6.15887,
     "end_time": "2020-12-15T19:53:25.684901",
     "exception": false,
     "start_time": "2020-12-15T19:53:19.526031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add Multiple layers of pooling, dropout and dense (fully-connected) layers with batch normalization\n",
    "inception_model = Sequential([\n",
    "        inception_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64,activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1,activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# define evaluation metrics\n",
    "METRICS = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "# compile the model\n",
    "inception_model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:53:31.192065Z",
     "iopub.status.busy": "2020-12-15T19:53:31.191428Z",
     "iopub.status.idle": "2020-12-15T19:56:46.274362Z",
     "shell.execute_reply": "2020-12-15T19:56:46.274935Z"
    },
    "papermill": {
     "duration": 197.869795,
     "end_time": "2020-12-15T19:56:46.275087",
     "exception": false,
     "start_time": "2020-12-15T19:53:28.405292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "im = inception_model.fit(train,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          class_weight=class_wgt,\n",
    "          steps_per_epoch=100,\n",
    "          validation_steps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T19:56:52.838841Z",
     "iopub.status.busy": "2020-12-15T19:56:52.834231Z",
     "iopub.status.idle": "2020-12-15T19:56:53.380374Z",
     "shell.execute_reply": "2020-12-15T19:56:53.379528Z"
    },
    "papermill": {
     "duration": 3.943768,
     "end_time": "2020-12-15T19:56:53.380512",
     "exception": false,
     "start_time": "2020-12-15T19:56:49.436744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(im.history['loss'], label='Loss')\n",
    "plt.plot(im.history['val_loss'], label='Val_Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(im.history['accuracy'], label='Accuracy')\n",
    "plt.plot(im.history['val_accuracy'], label='Val_Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "evaluation = inception_model.evaluate(test)\n",
    "# calculate accuarcy\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with train data\n",
    "evaluation = inception_model.evaluate(train)\n",
    "# calculate accuarcy\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output with test data using trained model\n",
    "pred = inception_model.predict(test, steps=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "print(confusion_matrix(test.classes, pred > 0.5))\n",
    "# compute classification report with threshold 0.5 and print\n",
    "pd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning - Proposed Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed models for pneumonia detection using chest x-ray images combines three transfer learning models: InceptionResNetV2, MobileNetV2, and EfficientNetB0. The output from the final layers of each model is concatenated and passed through a fully connected layer with a sigmoid activation function, which produces a probability score for pneumonia classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception-Residual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionResNetV2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "\n",
    "# build the model using InceptionResNetV2 from keras\n",
    "inception_resnet_base_model = InceptionResNetV2(input_shape=(180, 180, 3), include_top=False, weights='imagenet')\n",
    "inception_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Multiple layers of pooling, dropout and dense (fully-connected) layers with batch normalization\n",
    "inception_resnet_model = Sequential([\n",
    "        inception_resnet_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# define evaluation metrics\n",
    "METRICS = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "#compile the model\n",
    "inception_resnet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "im_resnet = inception_resnet_model.fit(train,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          class_weight=class_wgt,\n",
    "          steps_per_epoch=100,\n",
    "          validation_steps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(im_resnet.history['loss'], label='Loss')\n",
    "plt.plot(im_resnet.history['val_loss'], label='Val_Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(im_resnet.history['accuracy'], label='Accuracy')\n",
    "plt.plot(im_resnet.history['val_accuracy'], label='Val_Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "evaluation =inception_resnet_model.evaluate(test)\n",
    "# calculate accuracy\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with train data\n",
    "evaluation = inception_resnet_model.evaluate(train)\n",
    "# calculate accuracy\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output with test data using trained model\n",
    "pred = inception_model.predict(test, steps=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "print(confusion_matrix(test.classes, pred > 0.5))\n",
    "# compute classification report with threshold 0.5 and print\n",
    "pd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "\n",
    "# build the model using EfficientNetB0 from keras\n",
    "efficientnet_base_model = EfficientNetB0(input_shape=(180, 180, 3), include_top=False, weights='imagenet')\n",
    "inception_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Multiple layers of pooling, dropout and dense (fully-connected) layers with batch normalization\n",
    "efficientnet_model = Sequential([\n",
    "        efficientnet_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# define evaluation metrics\n",
    "METRICS = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "# compile the model\n",
    "efficientnet_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "ef_model = efficientnet_model.fit(train,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          class_weight=class_wgt,\n",
    "          steps_per_epoch=100,\n",
    "          validation_steps=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(ef_model.history['loss'], label='Loss')\n",
    "plt.plot(ef_model.history['val_loss'], label='Val_Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(ef_model.history['accuracy'], label='Accuracy')\n",
    "plt.plot(ef_model.history['val_accuracy'], label='Val_Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "evaluation =efficientnet_model.evaluate(test)\n",
    "# calculate accuracy\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with train data\n",
    "evaluation = efficientnet_model.evaluate(train)\n",
    "# calculate accuracy\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output with test data using trained model\n",
    "pred = inception_model.predict(test, steps=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "print(confusion_matrix(test.classes, pred > 0.5))\n",
    "# compute classification report with threshold 0.5 and print\n",
    "pd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobile Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Dropout\n",
    "\n",
    "# build the model using MobileNetV2 from keras\n",
    "mobilenetv2_base_model = MobileNetV2(input_shape=(180, 180, 3), include_top=False, weights='imagenet')\n",
    "inception_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Multiple layers of pooling, dropout and dense (fully-connected) layers with batch normalization\n",
    "mobilenetv2_model = Sequential([\n",
    "        mobilenetv2_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# define evaluation metrics\n",
    "METRICS = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    "\n",
    "# compile the model\n",
    "mobilenetv2_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for repeating validation data infinitely\n",
    "def repeat_generator(generator):\n",
    "    while True:\n",
    "        for batch in generator:\n",
    "            yield batch\n",
    "\n",
    "# Create a validation data generator that infinitely repeats the validation data\n",
    "validation_generator = repeat_generator(validation)\n",
    "\n",
    "# Fit the model using the validation data generator\n",
    "mnv2_model = mobilenetv2_model.fit(train,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          class_weight=class_wgt,\n",
    "          steps_per_epoch=100,\n",
    "          validation_steps=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the epochs vs loss graph of training and validation data\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(mnv2_model.history['loss'], label='Loss')\n",
    "plt.plot(mnv2_model.history['val_loss'], label='Val_Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Evolution')\n",
    "\n",
    "# Plot the epochs vs accuracy graph of training and validation data\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(mnv2_model.history['accuracy'], label='Accuracy')\n",
    "plt.plot(mnv2_model.history['val_accuracy'], label='Val_Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Evolution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "evaluation = mobilenetv2_model.evaluate(test)\n",
    "# calculate accuracy\n",
    "print(f\"Test Accuracy: {evaluation[1] * 100:.2f}%\")\n",
    "\n",
    "# evaluate the model with train data\n",
    "evaluation = mobilenetv2_model.evaluate(train)\n",
    "# calculate accuracy\n",
    "print(f\"Train Accuracy: {evaluation[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the output with test data using trained model\n",
    "pred = inception_model.predict(test, steps=len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confusion matrix with threshold 0.5 and print\n",
    "print(confusion_matrix(test.classes, pred > 0.5))\n",
    "# compute classification report with threshold 0.5 and print\n",
    "pd.DataFrame(classification_report(test.classes, pred > 0.5, output_dict=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that transfer learning models, particularly Inception-resnetv2, outperform the base CNN model in pneumonia diagnosis using chest X-ray images. This suggests that pre-trained models can be leveraged to improve the accuracy of medical image analysis tasks. Overall, the study provides promising results for the use of transfer learning models in pneumonia diagnosis using chest X-ray images, but further research is needed to address the limitations and improve the robustness of the method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "duration": 1787.894986,
   "end_time": "2020-12-15T19:59:10.837851",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-15T19:29:22.942865",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
